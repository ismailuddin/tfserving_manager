{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare to launch TensorFlow Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfserving_manager.model_server_config import TFServingModelServerConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Serving requires a special model config file in the protobuf message format. This file can be generated using the below functions. In the example below, the base path for the model is specified according to what it would be inside the Docker container where we will run TensorFlow Serving. Note, that the model base path does not include the model version sub-directory, as the model version can be specified inside the model config file. If no version is specified, the latest model is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = glob.glob(os.path.join(\"..\", \"models\", \"build\", \"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = TFServingModelServerConfig()\n",
    "config_file.initialise_blank_config()\n",
    "\n",
    "for model in models:\n",
    "    fp = pathlib.Path(model)\n",
    "    if fp.is_dir():\n",
    "        model_name = fp.stem\n",
    "        base_path = os.path.join(\"/\", \"models\", model_name, \"\")\n",
    "        config_file.add_model(model_name=model_name, base_path=base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file.save_protobuf(os.path.join(\"..\", \"models\", \"build\", \"model_config.proto\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch TensorFlow Serving in a Docker container using the following command from the root directory:\n",
    "\n",
    "```\n",
    "docker run \\\n",
    "-p 8500:8500 \\\n",
    "-p 8501:8501 \\\n",
    "--mount type=bind,source=\"$(pwd)/models/build/\",target=/models \\\n",
    "tensorflow/serving \\\n",
    "--model_config_file=/models/model_config.proto\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
